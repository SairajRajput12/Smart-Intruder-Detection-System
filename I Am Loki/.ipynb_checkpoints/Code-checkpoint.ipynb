{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a701bcb-a6a2-49fb-a189-de1aea06b058",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b49b5a38-0538-4a67-b26b-40e2328dc494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries succesfully imported\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import datetime\n",
    "from collections import deque\n",
    "from twilio.rest import Client \n",
    "import random\n",
    "import heapq\n",
    "\n",
    "\n",
    "print(\"Libraries succesfully imported\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de52c7e-00df-44cc-a2cb-a0e5f6be754c",
   "metadata": {},
   "source": [
    "# Step 1: Code for using ip webcam application and capturing the video fottages from the phone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77d694ff-95ec-4b6d-a9fc-50b6fad71919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Window normal so we can resize it\n",
    "cv2.namedWindow('frame', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Note the starting time\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize these variables for calculating FPS\n",
    "fps = 0 \n",
    "frame_counter = 0\n",
    "\n",
    "# Read the video steram from the camera\n",
    "cap = cv2.VideoCapture('https://192.168.43.1:8080/video') # this url must be pasted from the ip webcam application.\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break \n",
    "    \n",
    "    # Calculate the Average FPS\n",
    "    frame_counter += 1\n",
    "    fps = (frame_counter / (time.time() - start_time))\n",
    "    \n",
    "    # Display the FPS\n",
    "    cv2.putText(frame, 'FPS: {:.2f}'.format(fps), (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255),1)\n",
    "    \n",
    "    # Show the Frame\n",
    "    cv2.imshow('frame',frame)\n",
    "    \n",
    "    # Exit if q is pressed.\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release Capture and destroy windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96233618-45a9-41bb-884e-ccc35e556659",
   "metadata": {},
   "source": [
    "# Step 2: Twilio API For Messaging Purpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4818af66-9ef7-4184-af00-0b60ef30b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from twilio.rest import Client \n",
    "\n",
    "\n",
    "with open('credential.txt', 'r') as myfile:\n",
    "  data = myfile.read()\n",
    "\n",
    "# Convert data variable into dictionary\n",
    "info_dict = eval(data)\n",
    "\n",
    "# Your Account SID from twilio.com/console\n",
    "account_sid = info_dict['account_sid']\n",
    "\n",
    "# Your Auth Token from twilio.com/console\n",
    "auth_token  = info_dict['auth_token']\n",
    "\n",
    "# Set client and send the message\n",
    "client = Client(account_sid, auth_token)\n",
    "message = client.messages.create( to =info_dict['your_num'], from_ = info_dict['trial_num'], body= \"What's Up Man\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d534a6cf-6fc3-4cf7-a273-8c591d431d78",
   "metadata": {},
   "source": [
    "# Step 3: Building a Motion Detector with Background Subtraction and Contour detection:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e27b68-e1f4-46c1-b32f-74b7c4d695e8",
   "metadata": {},
   "source": [
    "## What are the background substraction methood ?? \n",
    "\n",
    "\n",
    "Basically these kinds of methods separate the background from the foreground in a video so for e.g. if a person walks in an empty room then the background subtraction algorithm would know thereâ€™s disturbance by subtracting the previously stored image of the room (without the person ) and the current image (with the person). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc31d4bb-5115-41ef-afc2-5c46893bcefb",
   "metadata": {},
   "source": [
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Background Subtraction Parameters for `cv2.createBackgroundSubtractorMOG2()`\r\n",
    "\r\n",
    "The `cv2.createBackgroundSubtractorMOG2()` function in OpenCV is used to create a background subtractor object based on the MOG2 algorithm. This function takes three arguments:\r\n",
    "\r\n",
    "1. **detectShadows (Boolean)**:\r\n",
    "   - This parameter enables or disables shadow detection.\r\n",
    "   - When set to `True`, the algorithm detects shadows in the video.\r\n",
    "   - Shadow detection can provide smoother and more robust results, but it may slightly decrease processing speed.\r\n",
    "\r\n",
    "2. **history (Integer)**:\r\n",
    "   - The `history` parameter determines the number of previous frames used to create the background model.\r\n",
    "   - Increasing this value can be beneficial if the target object frequently stops or pauses in the video.\r\n",
    "\r\n",
    "3. **Threshold Limit (Integer)**:\r\n",
    "   - The threshold limit helps filter out noise present in the frame.\r\n",
    "   - Increasing this value can help remove noise, such as white spots in the frame.\r\n",
    "   - Additionally, morphological operations like erosion can further eliminate noise from the frame.\r\n",
    "\r\n",
    "These parameters are essential for fine-tuning the background subtraction process and improving the accuracy of object detection itebook for easy reference.**\n",
    "\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11ef3d09-db0d-4da0-9718-af9e9166ba8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< cv2.BackgroundSubtractorMOG2 0000021F576324D0>\n"
     ]
    }
   ],
   "source": [
    "# For loading the video\n",
    "cap = cv2.VideoCapture('sample_video1.mp4') \n",
    "\n",
    "# Create the background subtractor object\n",
    "flog = cv2.createBackgroundSubtractorMOG2(detectShadows = True, varThreshold = 50, history = 2800)\n",
    "print(flog)\n",
    "\n",
    "while(True): \n",
    "    ret,frame = cap.read() \n",
    "\n",
    "    if not ret: \n",
    "        break\n",
    "\n",
    "     # Apply the background object on each frame\n",
    "    fgmask = flog.apply(frame) \n",
    "\n",
    "    # Get rid of the shadows\n",
    "    ret, fgmask = cv2.threshold(fgmask, 250, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "     # Show the background subtraction frame.\n",
    "    cv2.imshow('All three',fgmask)\n",
    "    k = cv2.waitKey(10)\n",
    "    if k == 27: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf06e5c-7d02-4821-b75e-aef2788c2443",
   "metadata": {},
   "source": [
    "# adding the dilation and erosion in this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec218486-15c5-45ee-96c5-6a0f0f71556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('sample_video1.mp4') \n",
    "\n",
    "# initialising the background substractor object \n",
    "backGroundSubstractorObject = cv2.createBackgroundSubtractorMOG2() \n",
    "thresh = 1100 \n",
    "\n",
    "# noise filter thresold \n",
    "kernel= None\n",
    "\n",
    "while(1): \n",
    "    ret,frame = cap.read() \n",
    "\n",
    "    if not ret: \n",
    "        break \n",
    "\n",
    "    # applying the dilation and erosion to the images\n",
    "    fgmask = backGroundSubstractorObject.apply(frame) \n",
    "\n",
    "    # Get rid of the shadows\n",
    "    ret, fgmask = cv2.threshold(fgmask, 250, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    \n",
    "    fgmask = cv2.erode(fgmask,kernel,iterations = 1)\n",
    "    fgmask = cv2.dilate(fgmask,kernel,iterations = 4)\n",
    "\n",
    "\n",
    "    # Detection of the contours in the frame \n",
    "    contours, hierarchy = cv2.findContours(fgmask,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    #  Contours in OpenCV refer to the outlines or boundaries of objects detected in an image,  \n",
    "    # represented as a series of connected points. They are useful for tasks such as object detection, shape analysis, and image segmentation.\n",
    "\n",
    "    if contours: \n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "        # This line of code finds the contour with the maximum area among the list of contours. The cv2.contourArea() function calculates the area of each contour, \n",
    "        # and max() is used to find the contour with the maximum area based on the values returned by cv2.contourArea(). The parameter key=cv2.contourArea specifies \n",
    "        # that the maximum value should be determined based on the area of the contours.\n",
    "        # Make sure the contour area is somewhat higher than a threshold to ensure it's a person and not noise.\n",
    "        \n",
    "        if cv2.contourArea(cnt) > thresh:\n",
    "            # Draw a bounding box around the person and label it as person detected\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "            cv2.putText(frame, 'Person Detected', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "        # Stack both frames and show the image\n",
    "    fgmask_3 = cv2.cvtColor(fgmask, cv2.COLOR_GRAY2BGR)\n",
    "    stacked = np.hstack((fgmask_3, frame))\n",
    "    # Adjust window size\n",
    "    cv2.imshow('Combined', cv2.resize(stacked, (800, 600)))\n",
    "\n",
    "    k = cv2.waitKey(40) \n",
    "    if k == ord('q'):\n",
    "        break\n",
    "        \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b93595-3675-4328-be37-9c5bcb180d62",
   "metadata": {},
   "source": [
    "# Step 4: Final code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf2d6fb-1269-4279-bdb2-a3ae698dbbe2",
   "metadata": {},
   "source": [
    "## Defining user defined function for the person detection and one function for the sending the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5137da5f-1e95-4e72-bc7b-d76aa5afdf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_person_present(frame, thresh=1100):\n",
    "    kernel = None\n",
    "    global foog\n",
    "    \n",
    "    # Apply background subtraction\n",
    "    fgmask = foog.apply(frame)\n",
    "\n",
    "    # Get rid of the shadows\n",
    "    ret, fgmask = cv2.threshold(fgmask, 250, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Apply some morphological operations to make sure you have a good mask\n",
    "    fgmask = cv2.dilate(fgmask, kernel, iterations=4)\n",
    "\n",
    "    # Detect contours in the frame\n",
    "    contours, hierarchy = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "     \n",
    "    # Check if there was a contour and the area is somewhat higher than some threshold so we know it's a person and not noise\n",
    "    if contours and cv2.contourArea(max(contours, key=cv2.contourArea)) > thresh:\n",
    "            \n",
    "        # Get the max contour\n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Draw a bounding box around the person and label it as person detected\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, 'Person Detected', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "        \n",
    "        return True, frame\n",
    "        \n",
    "        \n",
    "    # Otherwise report there was no one present\n",
    "    else:\n",
    "        return False, frame\n",
    "\n",
    "\n",
    "def send_message(body, info_dict):\n",
    "\n",
    "    # Your Account SID from twilio.com/console\n",
    "    account_sid = info_dict['account_sid']\n",
    "\n",
    "    # Your Auth Token from twilio.com/console\n",
    "    auth_token  = info_dict['auth_token']\n",
    "\n",
    "\n",
    "    client = Client(account_sid, auth_token)\n",
    "\n",
    "    message = client.messages.create(to=info_dict['your_num'], from_=info_dict['trial_num'], body=body)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_percentage_change(frame1, frame2):\n",
    "    # Convert frames to grayscale\n",
    "    gray_frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate absolute difference between the two frames\n",
    "    abs_diff = cv2.absdiff(gray_frame1, gray_frame2)\n",
    "\n",
    "    # Calculate percentage change\n",
    "    percentage_change = (np.count_nonzero(abs_diff) / abs_diff.size) * 100\n",
    "\n",
    "    return percentage_change\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8949b-4fa4-465e-9a13-24db65bb17fd",
   "metadata": {},
   "source": [
    "# Final Code With Message Sending + saving Frame Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f027d454-57af-41ec-b360-ca949e3cd39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set Window normal so we can resize it\n",
    "cv2.namedWindow('frame', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# This is a test video\n",
    "cap = cv2.VideoCapture('sample_video1.mp4')\n",
    "\n",
    "# Read the video stream from the camera\n",
    "# cap = cv2.VideoCapture('http://192.168.43.1:8080/video')\n",
    "\n",
    "# Get width and height of the frame\n",
    "width = int(cap.get(3))\n",
    "height = int(cap.get(4))\n",
    "\n",
    "# Read and store the credentials information in a dict\n",
    "with open('credential.txt', 'r') as myfile:\n",
    "  data = myfile.read()\n",
    "\n",
    "info_dict = eval(data)\n",
    "\n",
    "# Initialize the background Subtractor\n",
    "foog = cv2.createBackgroundSubtractorMOG2(detectShadows=True, varThreshold=100, history=2000)\n",
    "\n",
    "# Status is True when person is present and False when the person is not present.\n",
    "status = False\n",
    "\n",
    "# After the person disappears from view, wait at least 7 seconds before making the status False\n",
    "patience = 0.099\n",
    "\n",
    "# We don't consider an initial detection unless it's detected 15 times, this gets rid of false positives\n",
    "detection_thresh = 5\n",
    "\n",
    "# Initial time for calculating if patience time is up\n",
    "initial_time = None\n",
    "\n",
    "\n",
    "\n",
    "# We are creating a deque object of length detection_thresh and will store individual detection statuses here\n",
    "de = deque([False] * detection_thresh, maxlen=detection_thresh)\n",
    "\n",
    "# Initialize an empty heap to store frames along with their negated percentage change\n",
    "frame_heap = []\n",
    "\n",
    "\n",
    "# Initialize these variables for calculating FPS\n",
    "fps = 0 \n",
    "frame_counter = 0\n",
    "start_time = time.time()\n",
    "counter = 0\n",
    "\n",
    "ret,prevframe = cap.read()\n",
    "initial_frame = prevframe\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break \n",
    "    \n",
    "            \n",
    "    # This function will return a boolean variable telling if someone was present or not, it will also draw boxes if it \n",
    "    # finds someone\n",
    "    detected, annotated_image = is_person_present(frame)  \n",
    "    \n",
    "    # Register the current detection status on our deque object\n",
    "    de.appendleft(detected)\n",
    "    \n",
    "    # If we have consecutively detected a person 15 times then we are sure that someone is present    \n",
    "    # We also make this is the first time that this person has been detected so we only initialize the videowriter once\n",
    "    if sum(de) == detection_thresh and not status:                       \n",
    "            status = True\n",
    "            entry_time = datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")\n",
    "            # out = cv2.VideoWriter('outputs/{}.mp4'.format(entry_time), cv2.VideoWriter_fourcc(*'XVID'), 15.0, (width, height))\n",
    "\n",
    "    # If status is True but the person is not in the current frame\n",
    "    if status and not detected:\n",
    "        \n",
    "        # Restart the patience timer only if the person has not been detected for a few frames so we are sure it wasn't a \n",
    "        # False positive\n",
    "        print('i reached deepest of the darkest point')\n",
    "        if sum(de) > (detection_thresh/2): \n",
    "            if initial_time is None:\n",
    "                print('initial time as been setted to',initial_time)\n",
    "                initial_time = time.time()\n",
    "                \n",
    "            \n",
    "            elif initial_time is not None:        \n",
    "                print('now the initial time is not null beacuse current time is ',initial_time)\n",
    "                print(time.time() - initial_time)\n",
    "                # If the patience has run out and the person is still not detected then set the status to False\n",
    "                # Also save the video by releasing the video writer and send a text message.\n",
    "                if  time.time() - initial_time >= patience:\n",
    "                    status = False\n",
    "                    exit_time = datetime.datetime.now().strftime(\"%A, %I:%M:%S %p %d %B %Y\")\n",
    "                    # out.release()\n",
    "                    # print(out)\n",
    "                    initial_time = None\n",
    "                    de = deque([False] * len(de))\n",
    "                    body = \"Alert: \\nA Person Entered the Room at {} \\nLeft the room at {}\".format(entry_time, exit_time)\n",
    "                    print('message has been sent')\n",
    "                    send_message(body, info_dict)\n",
    "                    \n",
    "                \n",
    "    \n",
    "    # If a significant amount of detections (more than half of detection_thresh) has occurred then we reset the Initial Time.\n",
    "    elif status and sum(de) > (detection_thresh/2):\n",
    "        # print('person has detected beyond half thresold limit')\n",
    "        # print(sum(de))\n",
    "        change_percentage = calculate_percentage_change(prevframe, frame)\n",
    "        change_initial_change = calculate_percentage_change(initial_frame,frame)\n",
    "        if change_initial_change > 60: \n",
    "            heapq.heappush(frame_heap, (-change_percentage, frame,datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")))\n",
    "        prevframe = frame\n",
    "        initial_time = None\n",
    "    \n",
    "    # Get the current time in the required format\n",
    "    current_time = datetime.datetime.now().strftime(\"%A, %I:%M:%S %p %d %B %Y\")\n",
    "\n",
    "    # Display the FPS\n",
    "    cv2.putText(annotated_image, 'FPS: {:.2f}'.format(fps), (510, 450), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 40, 155), 2)\n",
    "    \n",
    "    # Display Time\n",
    "    cv2.putText(annotated_image, current_time, (310, 20), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1)    \n",
    "    \n",
    "    # Display the Room Status\n",
    "    cv2.putText(annotated_image, 'Room Occupied: {}'.format(str(status)), (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, \n",
    "                (200, 10, 150), 2)\n",
    "\n",
    "    # Show the patience Value\n",
    "    if initial_time is None:\n",
    "        text = 'Patience: {}'.format(patience)\n",
    "    else: \n",
    "        text = 'Patience: {:.2f}'.format(max(0, patience - (time.time() - initial_time)))\n",
    "        \n",
    "    cv2.putText(annotated_image, text, (10, 450), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 40, 155), 2)   \n",
    "\n",
    "    # If status is true save the frame\n",
    "    # if status:\n",
    "    #     out.write(annotated_image)\n",
    " \n",
    "    # Show the Frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    \n",
    "    # Calculate the Average FPS\n",
    "    frame_counter += 1\n",
    "    fps = (frame_counter / (time.time() - start_time))\n",
    "    \n",
    "    \n",
    "    # Exit if q is pressed.\n",
    "    if cv2.waitKey(30) == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "# Release Capture and destroy windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "frame_index = 0\n",
    "while frame_heap:\n",
    "    # Pop the frame with the maximum percentage change (negated)\n",
    "    neg_change_percentage, frame , date = heapq.heappop(frame_heap)\n",
    "    change_percentage = -neg_change_percentage\n",
    "    file_path = 'DetectedPerson'\n",
    "    # Save the frame to disk\n",
    "    cv2.imwrite(f'{file_path}/{date}.jpg', frame)\n",
    "    frame_index += 1\n",
    "# out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f56371-02f6-45de-b5d5-ff64623f8bd8",
   "metadata": {},
   "source": [
    "# Face Recognisation Code: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c56b1b-ad5a-4116-b362-412697630090",
   "metadata": {},
   "source": [
    "Post Link: https://data-flair.training/blogs/python-face-recognition/ \n",
    "\n",
    "Dlib installation Link:  https://www.youtube.com/watch?v=pHrgi8QLcKk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a1c8d3-5878-4a9d-8f8b-e39a62e8ba6e",
   "metadata": {},
   "source": [
    "# Code for Getting the person Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df72e924-4298-4f51-bbe6-fe24530a3482",
   "metadata": {},
   "outputs": [],
   "source": [
    "haar_file=cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "\n",
    "path=r'E:/MachineLearning/SmartIntruderDetectionSystem/I Am Loki/.ipynb_checkpoints/Face_Data/Wanda'\n",
    "width=130\n",
    "height=100  #saved image size = 130 x 100\n",
    "face_cascade = cv2.CascadeClassifier(haar_file)\n",
    "webcam=cv2.VideoCapture(0)\n",
    "\n",
    "count=0\n",
    "while count < 50:\n",
    "    i,im=webcam.read()\n",
    "    gray=cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_cascade.detectMultiScale(gray,1.3,4)\n",
    "    for (p,q,r,s) in faces:\n",
    "        cv2.rectangle(im,(p,q),(p+r,q+s),(255,0,0),2)\n",
    "        face=gray[q:q+s,p:p+r]\n",
    "        face_resize=cv2.resize(face,(width,height))\n",
    "        cv2.imwrite('%s/%s.png' % (path,count),face_resize)\n",
    "    count+=1\n",
    "    cv2.imshow('Saving DATA',im)\n",
    "    # cv2.imshow('Crop view',face)\n",
    "    cv2.waitKey(10)\n",
    "\n",
    "webcam.release()\n",
    "# Destroy the windows you created\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b203fe35-07e8-4ff2-a266-6008c6f8466d",
   "metadata": {},
   "source": [
    "# Code for recognising the person from our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f25d2629-74e4-4265-839a-a2ac62c23d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 9\n",
      "Line 25\n",
      "Line 29\n",
      "line 32\n",
      "I am Home lander\n",
      "(1, 125.43679061279639)\n",
      "(1, 127.72256132283984)\n",
      "(1, 115.46109628827851)\n",
      "(1, 119.74271255253026)\n",
      "(1, 112.40624569105138)\n",
      "(1, 111.2177535604939)\n",
      "(1, 109.36394582655733)\n",
      "(1, 111.08374440450845)\n",
      "(1, 111.08374440450845)\n",
      "(1, 110.07023956664395)\n",
      "(1, 110.07023956664395)\n",
      "(1, 109.5386538925592)\n",
      "(1, 112.9771122448242)\n",
      "(1, 114.11772487271158)\n",
      "(1, 111.54089256063861)\n",
      "(1, 111.20728415461743)\n",
      "(1, 110.9598129582645)\n",
      "(1, 109.15185597871267)\n",
      "(1, 109.15185597871267)\n",
      "(1, 110.5653756506295)\n",
      "(1, 109.12336733017432)\n",
      "(1, 109.33627279402965)\n",
      "(1, 109.33627279402965)\n",
      "(1, 109.66912078212482)\n",
      "(1, 112.45074797035781)\n",
      "(1, 113.01246978954877)\n",
      "(1, 117.10639770029317)\n",
      "(1, 118.5343429395242)\n",
      "(1, 120.52130043994502)\n",
      "(1, 119.62209954250856)\n",
      "(1, 119.52850804581449)\n",
      "(1, 119.32680134932781)\n",
      "(1, 119.40743969274143)\n",
      "(1, 110.08841039077187)\n",
      "(1, 106.2512490055933)\n",
      "(1, 104.35460447824214)\n",
      "(1, 103.43539370108596)\n",
      "(1, 102.27948718579847)\n",
      "(1, 101.89709473237382)\n",
      "(1, 101.95796881055905)\n",
      "(1, 104.34590181586715)\n",
      "(1, 104.65864960801805)\n",
      "(1, 106.70773577769683)\n",
      "(1, 107.91818293732939)\n",
      "(1, 109.70519038150785)\n",
      "(1, 109.70519038150785)\n",
      "(1, 106.59011543299384)\n",
      "(1, 109.33204942976026)\n",
      "(1, 109.33204942976026)\n",
      "(1, 108.0124387184869)\n",
      "(1, 107.4800070961074)\n",
      "(1, 107.4800070961074)\n",
      "(1, 109.72500884331494)\n",
      "(1, 109.72500884331494)\n",
      "(1, 109.56013471765286)\n",
      "(1, 110.84517868037233)\n",
      "(1, 110.84517868037233)\n",
      "(1, 110.3242546722953)\n",
      "(1, 110.25422023942734)\n",
      "(1, 110.25422023942734)\n",
      "(1, 109.96490051887719)\n",
      "(1, 111.33752064083156)\n",
      "(1, 108.40842424668116)\n",
      "(1, 108.98747895678768)\n",
      "(1, 110.1928523614443)\n",
      "(1, 108.35268506276242)\n",
      "(1, 110.0175665615134)\n",
      "(1, 109.69922884859695)\n",
      "(1, 109.61501067376582)\n",
      "(1, 110.36746662311957)\n",
      "(1, 110.37866052988018)\n",
      "(1, 110.39093222745176)\n",
      "(1, 110.32854025440025)\n",
      "(1, 110.32854025440025)\n",
      "(1, 110.17168674313693)\n",
      "(1, 109.55454722791733)\n",
      "(1, 108.08064782525531)\n",
      "(1, 108.08064782525531)\n",
      "(1, 109.7545693142139)\n",
      "(1, 109.7545693142139)\n",
      "(1, 109.80640633771038)\n",
      "(1, 109.41356537765111)\n",
      "(1, 109.41356537765111)\n",
      "(1, 108.0374283403326)\n",
      "(1, 108.58646265711157)\n",
      "(1, 108.58646265711157)\n",
      "(1, 107.91439195428786)\n",
      "(1, 109.20657748070145)\n",
      "(1, 111.94262861193334)\n",
      "(1, 108.65760739007257)\n",
      "(1, 107.5668362572164)\n",
      "(1, 110.29692732713599)\n",
      "(1, 109.79865630851059)\n",
      "(1, 108.52656717190331)\n",
      "(1, 110.23534930471325)\n",
      "(1, 109.69975409268594)\n",
      "(1, 109.12321541071219)\n",
      "(1, 111.04882501701418)\n",
      "(1, 109.30070474425264)\n",
      "(1, 106.73980687412462)\n",
      "(1, 109.57142799859264)\n",
      "(1, 109.50433636668991)\n",
      "(1, 107.7191208591966)\n",
      "(1, 109.22490023067128)\n",
      "(1, 108.45079516121358)\n",
      "(1, 110.96664026340615)\n",
      "(1, 110.96664026340615)\n",
      "(1, 109.3772803165974)\n",
      "(1, 108.77519995053905)\n",
      "(1, 109.12032778916297)\n",
      "(1, 112.49087333807473)\n",
      "(1, 110.05838780583127)\n",
      "(1, 110.71749392269932)\n",
      "(1, 111.24114955375526)\n",
      "(1, 108.87612480535952)\n",
      "(1, 108.87612480535952)\n",
      "(1, 110.21495590903822)\n",
      "(1, 112.5379136069281)\n",
      "(1, 112.5379136069281)\n",
      "(1, 109.69372670209056)\n",
      "(1, 109.1800758343027)\n",
      "(1, 109.1800758343027)\n",
      "(1, 108.28435160366215)\n",
      "(1, 107.26320807163833)\n",
      "(1, 104.34906134565401)\n",
      "(1, 105.52927245426194)\n",
      "(1, 105.52927245426194)\n",
      "(1, 104.54698816394658)\n",
      "(1, 106.72437480645462)\n",
      "(1, 108.67223262018429)\n",
      "(1, 108.97998674781817)\n",
      "(1, 107.50342604224504)\n",
      "(1, 107.73252184582059)\n",
      "(1, 110.21344212036706)\n",
      "(1, 109.74318975688807)\n",
      "(1, 105.02219289358108)\n",
      "(1, 109.43210992848061)\n",
      "(1, 115.59819443101551)\n",
      "(1, 114.84638425554529)\n",
      "(1, 116.19683475589139)\n",
      "(1, 116.75414240031355)\n",
      "(1, 116.73331331359175)\n",
      "(1, 117.93310446835704)\n",
      "(1, 119.67117205999398)\n",
      "(1, 118.2598256317093)\n",
      "(1, 116.90589592610014)\n",
      "(1, 116.90885161599927)\n",
      "(1, 116.8960050402772)\n",
      "(1, 117.64044946210173)\n",
      "(1, 113.49400271194226)\n",
      "(1, 112.61366204722114)\n",
      "(1, 111.60045576297787)\n",
      "(1, 111.00899418709477)\n",
      "(1, 112.33099813462776)\n",
      "(1, 111.07081006657752)\n",
      "(1, 111.79464956020213)\n",
      "(1, 114.10276490798265)\n",
      "(1, 116.99332950234482)\n",
      "(1, 119.62905592053042)\n",
      "(1, 118.49925516354544)\n",
      "(1, 117.80876726501806)\n",
      "(1, 116.3033890876313)\n",
      "(1, 115.58442951057252)\n",
      "(1, 111.78191180587862)\n",
      "(1, 111.78191180587862)\n",
      "(1, 105.941093167465)\n",
      "(1, 105.941093167465)\n",
      "(1, 103.67401949953387)\n",
      "(1, 103.8790996459846)\n",
      "(1, 105.43867917910009)\n",
      "(1, 105.43867917910009)\n",
      "(1, 109.38568028554896)\n",
      "(1, 108.68864461813594)\n",
      "(1, 107.4110477311963)\n",
      "(1, 107.4110477311963)\n",
      "(1, 109.23256112735623)\n",
      "(1, 111.26556351194155)\n",
      "(1, 115.89294053743099)\n",
      "(1, 114.16653521246744)\n",
      "(1, 114.72081541572705)\n",
      "(1, 112.80588325752306)\n",
      "(1, 114.13337448916263)\n",
      "(1, 113.46471492723863)\n",
      "(1, 112.02764172759184)\n",
      "(1, 112.99276358911746)\n",
      "(1, 111.39382163434134)\n",
      "(1, 111.34395713409188)\n",
      "(1, 107.31025539208473)\n",
      "(1, 109.86979295383782)\n",
      "(1, 110.17594803244808)\n",
      "(1, 110.11207713353853)\n",
      "(1, 110.37186699011716)\n",
      "(1, 110.68984752905688)\n",
      "(1, 110.68984752905688)\n",
      "(1, 111.43923938967042)\n",
      "(1, 111.43923938967042)\n",
      "(1, 110.14265459463674)\n",
      "(1, 109.85802677534107)\n",
      "(1, 109.42611753041578)\n",
      "(1, 112.45364166906037)\n",
      "(1, 113.11461986583781)\n",
      "(1, 109.67713442400283)\n",
      "(1, 110.02998842638547)\n",
      "(1, 107.50466895496542)\n",
      "(1, 105.60484630686909)\n",
      "(1, 103.53551062816027)\n",
      "(1, 106.14645309550903)\n",
      "(1, 108.71663060669539)\n",
      "(1, 108.92890522314251)\n",
      "(1, 106.61203670222305)\n",
      "(1, 105.72742819462296)\n",
      "(1, 105.52510842437005)\n",
      "(1, 106.46364539474658)\n",
      "(1, 112.15355644773979)\n",
      "(1, 120.20687603847644)\n",
      "(1, 116.94192965976931)\n",
      "(1, 115.45091720883396)\n",
      "(1, 120.37304135275346)\n",
      "(1, 119.55596160043613)\n",
      "(1, 119.55596160043613)\n",
      "(1, 118.05187358505138)\n",
      "(1, 120.16834143439475)\n",
      "(1, 118.89302284335389)\n",
      "(1, 118.08254474636814)\n",
      "(1, 117.37121031693091)\n",
      "(1, 116.49404800107119)\n",
      "(1, 112.97901814680404)\n",
      "(1, 113.70451701183455)\n",
      "(1, 113.12547658951193)\n",
      "(1, 113.12547658951193)\n",
      "(1, 110.98132447713223)\n",
      "(1, 110.98132447713223)\n",
      "(1, 108.25033308501412)\n",
      "(1, 105.10758145916796)\n",
      "(1, 105.18140328769863)\n",
      "(1, 108.34105735934514)\n",
      "(1, 108.70588786250677)\n",
      "(1, 108.53063406412197)\n",
      "(1, 108.47423929535935)\n",
      "(1, 107.49530732886748)\n",
      "(1, 107.02028181618132)\n",
      "(1, 108.20664320498132)\n",
      "(1, 109.37280822056262)\n",
      "(1, 109.37280822056262)\n",
      "(1, 112.14223792045809)\n",
      "(1, 112.14223792045809)\n",
      "(1, 110.29487587051923)\n",
      "(1, 107.91669040060937)\n",
      "(1, 109.81718318782163)\n",
      "(1, 109.61405503637731)\n",
      "(1, 109.14960418902008)\n",
      "(1, 109.04106527753943)\n",
      "(1, 107.89660224882138)\n",
      "(1, 108.77044724210995)\n",
      "(1, 107.40848047899625)\n",
      "(1, 108.81193285624327)\n",
      "(1, 107.7808300334431)\n",
      "(1, 107.7808300334431)\n",
      "(1, 109.98248475620532)\n",
      "(1, 109.98248475620532)\n",
      "(1, 109.1540005781748)\n",
      "(1, 107.54648218434941)\n",
      "(1, 107.54648218434941)\n",
      "(1, 108.72166032794364)\n",
      "(1, 107.94641464999994)\n",
      "(1, 108.55227688527684)\n",
      "(1, 107.44196352685951)\n",
      "(1, 109.87811224656322)\n",
      "(1, 108.80293012379946)\n",
      "(1, 106.64956168132741)\n",
      "(1, 110.13010083923685)\n",
      "(1, 108.1222912451262)\n",
      "(1, 108.81260868473056)\n",
      "(1, 108.33953356980493)\n",
      "(1, 104.56288258813156)\n",
      "(1, 109.02820848211259)\n",
      "(1, 109.02820848211259)\n",
      "(1, 112.70046443002431)\n",
      "(1, 112.70046443002431)\n",
      "(1, 124.76802385979153)\n",
      "(1, 124.76802385979153)\n",
      "(1, 113.2793588787352)\n",
      "(1, 116.66462013161598)\n",
      "(1, 116.66462013161598)\n",
      "(1, 112.32021380049335)\n",
      "(1, 107.56411938761313)\n",
      "(1, 107.131512846181)\n",
      "(1, 110.06455817194724)\n",
      "(1, 107.77841215232345)\n",
      "(1, 109.2922535691249)\n",
      "(1, 115.12907169930504)\n",
      "(1, 110.86853904362066)\n",
      "(1, 113.43253543603008)\n",
      "(1, 105.52414964052014)\n",
      "(1, 104.21192482018455)\n",
      "(1, 105.99578312613303)\n",
      "(1, 105.95221870957886)\n",
      "(1, 107.89647464982782)\n",
      "(1, 113.76270863710249)\n",
      "(1, 113.76270863710249)\n",
      "(1, 116.82337916406782)\n",
      "(1, 115.1384184462655)\n",
      "(1, 130.44360003673182)\n",
      "(1, 114.1576562947937)\n",
      "(1, 116.6584174308502)\n",
      "(1, 111.98184757802117)\n",
      "(1, 115.2154707959361)\n",
      "(1, 113.36361068217138)\n",
      "(1, 117.50383085706264)\n",
      "(1, 113.1391327199848)\n",
      "(1, 108.19970227957778)\n",
      "(1, 108.0368327617169)\n",
      "(1, 107.85321832286269)\n",
      "(1, 102.64649713035035)\n",
      "(1, 101.27508881929927)\n",
      "(1, 100.41018918861302)\n",
      "(1, 101.68752603492213)\n",
      "(1, 100.55994904831053)\n",
      "(1, 103.71763415377565)\n",
      "(1, 102.29889996424198)\n",
      "(1, 102.31668692900271)\n",
      "(1, 102.31668692900271)\n",
      "(1, 102.51823273835102)\n",
      "(1, 103.29670582091832)\n",
      "(1, 105.65554746160682)\n",
      "(1, 106.42575237286553)\n",
      "(1, 107.84324621305264)\n",
      "(1, 107.43308293754298)\n",
      "(1, 107.43308293754298)\n",
      "(1, 104.36563347814646)\n",
      "(1, 104.36563347814646)\n",
      "(1, 108.25505519091922)\n",
      "(1, 108.25505519091922)\n",
      "(1, 107.34801930569714)\n",
      "(1, 106.36317546036375)\n",
      "(1, 104.08279585363806)\n",
      "(1, 104.08279585363806)\n",
      "(1, 105.24049722360162)\n",
      "(1, 104.77247706405532)\n",
      "(1, 106.85517088324285)\n",
      "(1, 105.37020435947257)\n",
      "(1, 105.40979533358777)\n",
      "(1, 105.91376840553609)\n",
      "(1, 105.58339693525316)\n",
      "(1, 103.24346860782794)\n",
      "(1, 103.42723241136373)\n",
      "(1, 101.8429831430275)\n",
      "(1, 103.96271776580416)\n",
      "(1, 102.59866795968381)\n",
      "(1, 103.38762445040415)\n",
      "(1, 100.64789344852319)\n",
      "(1, 100.47906433704107)\n",
      "(1, 100.06845561506397)\n",
      "(1, 100.66765538633753)\n",
      "(1, 100.91629891769809)\n",
      "(1, 100.63463560318284)\n",
      "(1, 100.06383663687556)\n",
      "(1, 100.06383663687556)\n",
      "(1, 102.60695412703271)\n",
      "(1, 101.04697165836232)\n",
      "(1, 102.68102819984522)\n",
      "(1, 104.57821682877947)\n",
      "(1, 103.36307356707586)\n",
      "(1, 101.8996028731334)\n",
      "(1, 100.78423760896584)\n",
      "(1, 103.25678020692183)\n",
      "(1, 104.3846469997657)\n",
      "(1, 103.04358960198022)\n",
      "(1, 103.57257784846387)\n",
      "(1, 102.7863196653538)\n",
      "(1, 101.91827780691266)\n",
      "(1, 104.19219073209766)\n",
      "(1, 104.19219073209766)\n",
      "(1, 101.91013108824971)\n",
      "(1, 101.91013108824971)\n",
      "(1, 101.88669193603562)\n",
      "(1, 103.02793682608072)\n",
      "(1, 101.86723847001771)\n",
      "(1, 102.8606187108897)\n",
      "(1, 104.64486457437465)\n",
      "(1, 102.70048520054223)\n",
      "(1, 103.97299872127743)\n",
      "(1, 103.27495502036457)\n",
      "(1, 102.96272346762709)\n",
      "(1, 102.9724889901864)\n",
      "(1, 102.29247828865249)\n",
      "(1, 103.00829801745938)\n",
      "(1, 103.85580363858672)\n",
      "(1, 102.17794794712897)\n",
      "(1, 103.99954389128739)\n",
      "(1, 103.46150304275099)\n",
      "(1, 101.29417967720977)\n",
      "(1, 103.01114853285718)\n",
      "(1, 102.31535815508545)\n",
      "(1, 102.74540898712822)\n",
      "(1, 100.77610110811071)\n",
      "(1, 103.22529852812434)\n",
      "(1, 103.62677167632845)\n",
      "(1, 101.73656210688591)\n",
      "(1, 103.69727872287842)\n",
      "(1, 103.34254312965129)\n",
      "(1, 102.64952254971226)\n",
      "(1, 102.73609812325559)\n",
      "(1, 102.48255212908225)\n",
      "(1, 103.15928711066302)\n",
      "(1, 103.61008940867826)\n",
      "(1, 103.26807809540325)\n",
      "(1, 103.26807809540325)\n",
      "(1, 105.09769770256206)\n",
      "(1, 105.09769770256206)\n",
      "(1, 104.25976211750508)\n",
      "(1, 104.06283424429189)\n",
      "(1, 104.06283424429189)\n",
      "(1, 104.50952969016232)\n",
      "(1, 102.76161126151831)\n",
      "(1, 102.76161126151831)\n",
      "(1, 104.1645634754937)\n",
      "(1, 104.1645634754937)\n",
      "(1, 104.27170631893752)\n",
      "(1, 104.13260820861203)\n",
      "(1, 103.88343054130539)\n",
      "(1, 103.88343054130539)\n",
      "(1, 105.39295130190025)\n",
      "(1, 106.67384878191702)\n",
      "(1, 106.67384878191702)\n",
      "(1, 104.59173960221592)\n",
      "(1, 103.78819198411671)\n",
      "(1, 103.78819198411671)\n",
      "(1, 105.24069703343035)\n",
      "(1, 105.8011565400515)\n",
      "(1, 106.76668947513711)\n",
      "(1, 106.76668947513711)\n",
      "(1, 106.40458272606324)\n",
      "(1, 104.8091400285746)\n",
      "(1, 104.8091400285746)\n",
      "(1, 106.59780624901016)\n",
      "(1, 108.49139670487963)\n",
      "(1, 107.88548751426245)\n",
      "(1, 109.94366575825975)\n",
      "(1, 105.03019232689435)\n",
      "(1, 107.2754717670149)\n",
      "(1, 108.70950067427448)\n",
      "(1, 109.20943043663252)\n",
      "(1, 109.67231688508309)\n",
      "(1, 110.09682410623698)\n",
      "(1, 107.37944839159134)\n",
      "(1, 109.68337107430821)\n",
      "(1, 112.88586373196192)\n",
      "(1, 112.14958917842871)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m face_resize \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(face, (width, height))\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Try to recognize the face\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_resize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(prediction)\n\u001b[0;32m     48\u001b[0m cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (x, y), (x \u001b[38;5;241m+\u001b[39m w, y \u001b[38;5;241m+\u001b[39m h), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "os.chdir(r'E:/MachineLearning/SmartIntruderDetectionSystem/I Am Loki/.ipynb_checkpoints')\n",
    "haar_file = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "datasets = 'Face_Data'\n",
    "\n",
    "print('Line 9')\n",
    "# Create a list of images and a list of corresponding names\n",
    "(images, labels, names, id) = ([], [], {}, 0)\n",
    "for (subdirs, dirs, files) in os.walk(datasets):\n",
    "    for subdir in dirs:\n",
    "        names[id] = subdir\n",
    "        subjectpath = os.path.join(datasets, subdir)\n",
    "        for filename in os.listdir(subjectpath):\n",
    "            path = os.path.join(subjectpath, filename)\n",
    "            label = id\n",
    "            images.append(cv2.imread(path, 0))\n",
    "            labels.append(int(label))\n",
    "        id += 1\n",
    "\n",
    "width, height = 130, 100\n",
    "\n",
    "print('Line 25')\n",
    "# Create a Numpy array \n",
    "(images, labels) = [numpy.array(lst) for lst in [images, labels]]\n",
    "\n",
    "print('Line 29')\n",
    "# Train Model\n",
    "model = cv2.face.LBPHFaceRecognizer_create()\n",
    "print('line 32')\n",
    "model.train(images, labels)\n",
    "\n",
    "print('I am Home lander')\n",
    "# Face Recognition \n",
    "face_cascade = cv2.CascadeClassifier(haar_file)\n",
    "webcam = cv2.VideoCapture(0)\n",
    "c = 1\n",
    "while True:\n",
    "    ret,frame = webcam.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 4)\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = gray[y:y + h, x:x + w]\n",
    "        face_resize = cv2.resize(face, (width, height))\n",
    "        # Try to recognize the face\n",
    "        prediction = model.predict(face_resize)\n",
    "        print(prediction)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        if prediction[1] < 100:\n",
    "            # print('deteccted')\n",
    "            cv2.putText(frame, '%s' % (names[prediction[0]]), (x-10, y-10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2, (0, 255, 0))\n",
    "        else:\n",
    "            cv2.putText(frame, 'not recognized', (x-10, y-10), cv2.FONT_HERSHEY_PLAIN, 1, (0, 255, 0))\n",
    "        \n",
    "    # Display image using matplotlib\n",
    "    # print(im)\n",
    "    cv2.imshow('recognised',frame)\n",
    "    key = cv2.waitKey(10)\n",
    "    if key == 'a':\n",
    "        break\n",
    "\n",
    "webcam.release()\n",
    "# Destroy the windows you created\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e91705-261a-4bfe-b910-a6180444aebd",
   "metadata": {},
   "source": [
    "# Integrating our code with facial recognition code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42400fe-ccbb-4354-803f-9d2ae567edaf",
   "metadata": {},
   "source": [
    "# Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce0df61-0607-4c24-b4b8-a16767bd73c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries succesfully imported\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import datetime\n",
    "from collections import deque\n",
    "from twilio.rest import Client \n",
    "import random\n",
    "import heapq\n",
    "\n",
    "\n",
    "print(\"Libraries succesfully imported\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aec6e66-0d90-4e93-95b7-69f4084f06ac",
   "metadata": {},
   "source": [
    "# Initialise our data set and creating our first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "955dfb65-ff6f-4ae2-9be5-2a91845671c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 9\n",
      "Line 25\n",
      "Line 29\n",
      "line 32\n",
      "I am Home lander\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.chdir(r'E:/MachineLearning/SmartIntruderDetectionSystem/I Am Loki/.ipynb_checkpoints')\n",
    "haar_file = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "datasets = 'Face_Data'\n",
    "\n",
    "print('Line 9')\n",
    "# Create a list of images and a list of corresponding names\n",
    "(images, labels, names, id) = ([], [], {}, 0)\n",
    "for (subdirs, dirs, files) in os.walk(datasets):\n",
    "    # from this for loop we are traversing through each directory which contains the subdirectory\n",
    "    # names of subdirectory is assigned with the current name \n",
    "    \n",
    "    for subdir in dirs:\n",
    "        names[id] = subdir\n",
    "        subjectpath = os.path.join(datasets, subdir)\n",
    "        for filename in os.listdir(subjectpath):\n",
    "            path = os.path.join(subjectpath, filename)\n",
    "            # print(\"Joined Path:\", path)  # Print the joined path\n",
    "            label = id\n",
    "            images.append(cv2.imread(path, 0))\n",
    "            labels.append(int(label))\n",
    "        id += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "width, height = 130, 100\n",
    "\n",
    "print('Line 25')\n",
    "# Create a Numpy array \n",
    "(images, labels) = [numpy.array(lst) for lst in [images, labels]]\n",
    "\n",
    "print('Line 29')\n",
    "# Train Model\n",
    "model = cv2.face.LBPHFaceRecognizer_create()\n",
    "print('line 32')\n",
    "model.train(images, labels)\n",
    "\n",
    "print('I am Home lander')\n",
    "# Face Recognition \n",
    "face_cascade = cv2.CascadeClassifier(haar_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca148dd5-221a-4838-8eca-6e0a06621660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no error in functions\n"
     ]
    }
   ],
   "source": [
    "def is_person_present(frame, thresh=1100):\n",
    "    kernel = None\n",
    "    global foog\n",
    "    \n",
    "    # Apply background subtraction\n",
    "    fgmask = foog.apply(frame)\n",
    "\n",
    "    # Get rid of the shadows\n",
    "    ret, fgmask = cv2.threshold(fgmask, 250, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Apply some morphological operations to make sure you have a good mask\n",
    "    fgmask = cv2.dilate(fgmask, kernel, iterations=4)\n",
    "\n",
    "    # Detect contours in the frame\n",
    "    contours, hierarchy = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "     \n",
    "    # Check if there was a contour and the area is somewhat higher than some threshold so we know it's a person and not noise\n",
    "    if contours and cv2.contourArea(max(contours, key=cv2.contourArea)) > thresh:\n",
    "            \n",
    "        # Get the max contour\n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Draw a bounding box around the person and label it as person detected\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, 'Person Detected', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "        \n",
    "        return True, frame\n",
    "        \n",
    "        \n",
    "    # Otherwise report there was no one present\n",
    "    else:\n",
    "        return False, frame\n",
    "\n",
    "\n",
    "def send_message(body, info_dict):\n",
    "\n",
    "    # Your Account SID from twilio.com/console\n",
    "    account_sid = info_dict['account_sid']\n",
    "\n",
    "    # Your Auth Token from twilio.com/console\n",
    "    auth_token  = info_dict['auth_token']\n",
    "\n",
    "\n",
    "    client = Client(account_sid, auth_token)\n",
    "\n",
    "    message = client.messages.create(to=info_dict['your_num'], from_=info_dict['trial_num'], body=body)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_percentage_change(frame1, frame2):\n",
    "    # Convert frames to grayscale\n",
    "    gray_frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate absolute difference between the two frames\n",
    "    abs_diff = cv2.absdiff(gray_frame1, gray_frame2)\n",
    "\n",
    "    # Calculate percentage change\n",
    "    percentage_change = (np.count_nonzero(abs_diff) / abs_diff.size) * 100\n",
    "\n",
    "    return percentage_change\n",
    "\n",
    "def recogniseFromFace(frame): \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 4)\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = gray[y:y + h, x:x + w]\n",
    "        face_resize = cv2.resize(face, (width, height))\n",
    "        # Try to recognize the face\n",
    "        prediction = model.predict(face_resize)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        if prediction[1] < 500:\n",
    "            # print('deteccted')\n",
    "            cv2.putText(frame, '%s' % (names[prediction[0]]), (x-10, y-10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2, (0, 255, 0))\n",
    "            return True ,names[prediction[0]]\n",
    "        else:\n",
    "            cv2.putText(frame, 'not recognized', (x-10, y-10), cv2.FONT_HERSHEY_PLAIN, 1, (0, 255, 0))\n",
    "            return False ,\"not recognized\"\n",
    "\n",
    "print('no error in functions')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b310da9-6dfd-4db4-af82-c52b93685e17",
   "metadata": {},
   "source": [
    "# What is the use of os path join function ?? \n",
    "- it is used for the common path name manipulation.\n",
    "\n",
    "**Syntax:** os.path.join(path, *paths) \r\n",
    "\n",
    "\n",
    "\r\n",
    "Parameter\n",
    "\n",
    "path: A path-like object representing a file system path. \n",
    "\n",
    "\n",
    "   \n",
    "*path: A path-like object representing a file system path. It represents the path components to be joined. A path-like object is either a string or bytes object representing a path.ath.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d3026f-2161-4e27-b450-ce2405c8e3d2",
   "metadata": {},
   "source": [
    "The os.path.join() Method in Python joins one or more path components intelligently. This method concatenates various path components with exactly one directory separator (â€˜/â€™) following each non-empty part except the last path component. If the last path component to be joined is empty then a directory separator (â€˜/â€™) is put at the end. \r\n",
    "\r\n",
    "If a path component represents an absolute path, then all previous components joined are discarded, and joining continues from the absolute path component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7621e7e0-ae51-4a87-b691-2d2a57db2c3e",
   "metadata": {},
   "source": [
    "# What is the predict methood ??\n",
    "\n",
    "The predict() method in OpenCV's face recognition models, such as LBPHFaceRecognizer, returns a tuple containing the predicted label and the confidence score associated with the prediction.\r\n",
    "\r\n",
    "Here's a breakdown:\r\n",
    "\r\n",
    "Predicted Label: This is an integer value representing the predicted label or identity of the face detected. Each unique person in the training dataset is assigned a unique label.\r\n",
    "\r\n",
    "Confidence Score: This is a floating-point value representing the confidence level or certainty of the prediction. Lower scores indicate higher confidence in the prediction, while higher scores indicate lower confidence.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11100fef-1204-4df3-b11a-d490ead2b40d",
   "metadata": {},
   "source": [
    "# Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1350a4-44fe-42fb-afe4-e825831ae221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.85204475308642\n",
      "26.0649112654321\n",
      "25.807146990740744\n",
      "25.62138310185185\n",
      "26.02001350308642\n",
      "26.419029706790127\n",
      "27.086082175925924\n",
      "57.253568672839506\n",
      "20.536747685185187\n",
      "33.28780864197531\n",
      "22.676601080246915\n",
      "26.152874228395063\n",
      "26.882282021604937\n",
      "27.312885802469133\n",
      "27.16034915123457\n",
      "27.36405285493827\n",
      "29.12379436728395\n",
      "29.433786651234566\n",
      "27.94212962962963\n",
      "28.692274305555554\n",
      "28.92558834876543\n",
      "25.2265625\n",
      "32.36805555555556\n",
      "24.506028163580247\n",
      "32.73784722222222\n",
      "26.154610339506174\n",
      "33.71672453703704\n",
      "26.14776234567901\n",
      "32.70539158950617\n",
      "25.701244212962965\n",
      "33.29402970679013\n",
      "23.802854938271604\n",
      "29.678867669753085\n",
      "29.951822916666664\n",
      "27.996961805555554\n",
      "27.425009645061728\n",
      "59.29335455246913\n",
      "21.78144290123457\n",
      "36.517119984567906\n",
      "22.09153163580247\n",
      "36.79470486111111\n",
      "21.85281635802469\n",
      "28.950327932098762\n",
      "27.770785108024693\n",
      "28.101032021604937\n",
      "27.56331983024691\n",
      "27.45900848765432\n",
      "26.888503086419753\n",
      "27.129292052469133\n",
      "26.34490740740741\n",
      "27.12620563271605\n",
      "27.14916087962963\n",
      "26.314284336419753\n",
      "25.590422453703702\n",
      "26.078125000000004\n",
      "25.535252700617285\n",
      "26.216145833333332\n",
      "25.580632716049383\n",
      "25.019386574074076\n",
      "30.424672067901238\n",
      "22.931423611111114\n",
      "31.039158950617285\n",
      "22.98119212962963\n",
      "31.31274112654321\n",
      "22.56076388888889\n",
      "56.943335262345684\n",
      "20.028211805555557\n",
      "33.65403163580247\n",
      "19.002555941358025\n",
      "25.385802469135804\n",
      "29.90258487654321\n",
      "22.91579861111111\n",
      "29.5477912808642\n",
      "22.44685570987654\n",
      "29.821759259259263\n",
      "22.8897087191358\n",
      "29.892505787037038\n",
      "23.00636574074074\n",
      "30.24512924382716\n",
      "23.356963734567902\n",
      "38.05796682098765\n",
      "35.17838541666667\n",
      "45.75853587962963\n",
      "42.30160108024691\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set Window normal so we can resize it\n",
    "cv2.namedWindow('frame', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# This is a test video\n",
    "cap = cv2.VideoCapture('sample_video1.mp4')\n",
    "\n",
    "# Read the video stream from the camera\n",
    "# cap = cv2.VideoCapture('http://192.168.43.1:8080/video')\n",
    "\n",
    "# Get width and height of the frame\n",
    "width = int(cap.get(3))\n",
    "height = int(cap.get(4))\n",
    "\n",
    "# Read and store the credentials information in a dict\n",
    "with open('credential.txt', 'r') as myfile:\n",
    "  data = myfile.read()\n",
    "\n",
    "info_dict = eval(data)\n",
    "\n",
    "# Initialize the background Subtractor\n",
    "foog = cv2.createBackgroundSubtractorMOG2(detectShadows=True, varThreshold=100, history=2000)\n",
    "\n",
    "# Status is True when person is present and False when the person is not present.\n",
    "status = False\n",
    "\n",
    "# After the person disappears from view, wait at least 7 seconds before making the status False\n",
    "patience = 0.099\n",
    "\n",
    "# We don't consider an initial detection unless it's detected 15 times, this gets rid of false positives\n",
    "detection_thresh = 5\n",
    "\n",
    "# Initial time for calculating if patience time is up\n",
    "initial_time = None\n",
    "\n",
    "\n",
    "\n",
    "# We are creating a deque object of length detection_thresh and will store individual detection statuses here\n",
    "de = deque([False] * detection_thresh, maxlen=detection_thresh)\n",
    "\n",
    "# Initialize an empty heap to store frames along with their negated percentage change\n",
    "frame_heap = []\n",
    "\n",
    "\n",
    "# Initialize these variables for calculating FPS\n",
    "fps = 0 \n",
    "frame_counter = 0\n",
    "start_time = time.time()\n",
    "counter = 0\n",
    "\n",
    "ret,prevframe = cap.read()\n",
    "initial_frame = prevframe\n",
    "recognisedName = \"\"\n",
    "sent  = False\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break \n",
    "    \n",
    "            \n",
    "    # This function will return a boolean variable telling if someone was present or not, it will also draw boxes if it \n",
    "    # finds someone\n",
    "    detected, annotated_image = is_person_present(frame)  \n",
    "    # print(recognisedName)\n",
    "    if detected and len(recognisedName) == 0: \n",
    "        result = recogniseFromFace(frame)  \n",
    "        if result is not None: \n",
    "            val,name = result \n",
    "            if val: \n",
    "                recognisedName = name \n",
    "            else: \n",
    "                recognisedName = \"\"\n",
    "    # Register the current detection status on our deque object\n",
    "    de.appendleft(detected)\n",
    "    \n",
    "    # If we have consecutively detected a person 15 times then we are sure that someone is present    \n",
    "    # We also make this is the first time that this person has been detected so we only initialize the videowriter once\n",
    "    if sum(de) == detection_thresh and not status:                       \n",
    "            status = True\n",
    "            entry_time = datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")\n",
    "            # out = cv2.VideoWriter('outputs/{}.mp4'.format(entry_time), cv2.VideoWriter_fourcc(*'XVID'), 15.0, (width, height))\n",
    "\n",
    "    # If status is True but the person is not in the current frame\n",
    "    if status and not detected:\n",
    "        \n",
    "        # Restart the patience timer only if the person has not been detected for a few frames so we are sure it wasn't a \n",
    "        # False positive\n",
    "        print('i reached deepest of the darkest point')\n",
    "        if sum(de) > (detection_thresh/2): \n",
    "            if initial_time is None:\n",
    "                print('initial time as been setted to',initial_time)\n",
    "                initial_time = time.time()\n",
    "                \n",
    "            \n",
    "            elif initial_time is not None:        \n",
    "                print('now the initial time is not null beacuse current time is ',initial_time)\n",
    "                print(time.time() - initial_time)\n",
    "                # If the patience has run out and the person is still not detected then set the status to False\n",
    "                # Also save the video by releasing the video writer and send a text message.\n",
    "                if  time.time() - initial_time >= patience:\n",
    "                    status = False\n",
    "                    print(\"inside the loop\")\n",
    "                    exit_time = datetime.datetime.now().strftime(\"%A, %I:%M:%S %p %d %B %Y\")\n",
    "                    # out.release()\n",
    "                    # print(out)\n",
    "                    initial_time = None\n",
    "                    de = deque([False] * len(de))\n",
    "                    body = \"Alert: \\nA Person Entered the Room at {} \\nLeft the room at {}\".format(entry_time, exit_time) \n",
    "                    if len(recognisedName) > 0: \n",
    "                        body = \"Alert: {} Has entered in your Room at {} \\n Left the room at {}\".format(recognisedName,entry_time,exit_time)\n",
    "                    print('message has been sent')\n",
    "                    send_message(body, info_dict)\n",
    "                    \n",
    "                \n",
    "    \n",
    "    # If a significant amount of detections (more than half of detection_thresh) has occurred then we reset the Initial Time.\n",
    "    elif status and sum(de) > (detection_thresh/2):\n",
    "        change_percentage = calculate_percentage_change(prevframe, frame)\n",
    "        change_initial_change = calculate_percentage_change(initial_frame,frame)\n",
    "        print(change_percentage)\n",
    "        if change_percentage > 95 and sent == False: \n",
    "            sent = True\n",
    "            body = \"Alert: \\n Some one has changed the camera configuration at:{}\".format(datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")) \n",
    "            send_message(body,info_dict)\n",
    "    \n",
    "        if change_initial_change > 60: \n",
    "            heapq.heappush(frame_heap, (-change_percentage, frame,datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")))\n",
    "        prevframe = frame\n",
    "        initial_time = None\n",
    "    \n",
    "    # Get the current time in the required format\n",
    "    current_time = datetime.datetime.now().strftime(\"%A, %I:%M:%S %p %d %B %Y\")\n",
    "\n",
    "    # Display the FPS\n",
    "    cv2.putText(annotated_image, 'FPS: {:.2f}'.format(fps), (510, 450), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 40, 155), 2)\n",
    "    \n",
    "    # Display Time\n",
    "    cv2.putText(annotated_image, current_time, (310, 20), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1)    \n",
    "    \n",
    "    # Display the Room Status\n",
    "    cv2.putText(annotated_image, 'Room Occupied: {}'.format(str(status)), (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, \n",
    "                (200, 10, 150), 2)\n",
    "\n",
    "    # Show the patience Value\n",
    "    if initial_time is None:\n",
    "        text = 'Patience: {}'.format(patience)\n",
    "    else: \n",
    "        text = 'Patience: {:.2f}'.format(max(0, patience - (time.time() - initial_time)))\n",
    "        \n",
    "    cv2.putText(annotated_image, text, (10, 450), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 40, 155), 2)   \n",
    "\n",
    " \n",
    "    # Show the Frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    \n",
    "    # Calculate the Average FPS\n",
    "    frame_counter += 1\n",
    "    fps = (frame_counter / (time.time() - start_time))\n",
    "    \n",
    "    \n",
    "    # Exit if q is pressed.\n",
    "    if cv2.waitKey(30) == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "# Release Capture and destroy windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "frame_index = 0\n",
    "while frame_heap:\n",
    "    # Pop the frame with the maximum percentage change (negated)\n",
    "    neg_change_percentage, frame , date = heapq.heappop(frame_heap)\n",
    "    change_percentage = -neg_change_percentage\n",
    "    file_path = 'DetectedPerson'\n",
    "    # Save the frame to disk\n",
    "    cv2.imwrite(f'{file_path}/{date}.jpg', frame)\n",
    "    frame_index += 1\n",
    "# out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d61d2c0-d534-47d4-aeca-31d307e9dcbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ba24e8-1147-4a68-a13c-8a5479d7bd55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
