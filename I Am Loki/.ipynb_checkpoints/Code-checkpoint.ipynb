{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a701bcb-a6a2-49fb-a189-de1aea06b058",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b49b5a38-0538-4a67-b26b-40e2328dc494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries succesfully imported\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import datetime\n",
    "from collections import deque\n",
    "from twilio.rest import Client \n",
    "import random\n",
    "import heapq\n",
    "\n",
    "\n",
    "print(\"Libraries succesfully imported\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de52c7e-00df-44cc-a2cb-a0e5f6be754c",
   "metadata": {},
   "source": [
    "# Step 1: Code for using ip webcam application and capturing the video fottages from the phone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77d694ff-95ec-4b6d-a9fc-50b6fad71919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Window normal so we can resize it\n",
    "cv2.namedWindow('frame', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Note the starting time\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize these variables for calculating FPS\n",
    "fps = 0 \n",
    "frame_counter = 0\n",
    "\n",
    "# Read the video steram from the camera\n",
    "cap = cv2.VideoCapture('https://192.168.43.1:8080/video') # this url must be pasted from the ip webcam application.\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break \n",
    "    \n",
    "    # Calculate the Average FPS\n",
    "    frame_counter += 1\n",
    "    fps = (frame_counter / (time.time() - start_time))\n",
    "    \n",
    "    # Display the FPS\n",
    "    cv2.putText(frame, 'FPS: {:.2f}'.format(fps), (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255),1)\n",
    "    \n",
    "    # Show the Frame\n",
    "    cv2.imshow('frame',frame)\n",
    "    \n",
    "    # Exit if q is pressed.\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release Capture and destroy windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96233618-45a9-41bb-884e-ccc35e556659",
   "metadata": {},
   "source": [
    "# Step 2: Twilio API For Messaging Purpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4818af66-9ef7-4184-af00-0b60ef30b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from twilio.rest import Client \n",
    "\n",
    "\n",
    "with open('credential.txt', 'r') as myfile:\n",
    "  data = myfile.read()\n",
    "\n",
    "# Convert data variable into dictionary\n",
    "info_dict = eval(data)\n",
    "\n",
    "# Your Account SID from twilio.com/console\n",
    "account_sid = info_dict['account_sid']\n",
    "\n",
    "# Your Auth Token from twilio.com/console\n",
    "auth_token  = info_dict['auth_token']\n",
    "\n",
    "# Set client and send the message\n",
    "client = Client(account_sid, auth_token)\n",
    "message = client.messages.create( to =info_dict['your_num'], from_ = info_dict['trial_num'], body= \"What's Up Man\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d534a6cf-6fc3-4cf7-a273-8c591d431d78",
   "metadata": {},
   "source": [
    "# Step 3: Building a Motion Detector with Background Subtraction and Contour detection:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e27b68-e1f4-46c1-b32f-74b7c4d695e8",
   "metadata": {},
   "source": [
    "## What are the background substraction methood ?? \n",
    "\n",
    "\n",
    "Basically these kinds of methods separate the background from the foreground in a video so for e.g. if a person walks in an empty room then the background subtraction algorithm would know thereâ€™s disturbance by subtracting the previously stored image of the room (without the person ) and the current image (with the person). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc31d4bb-5115-41ef-afc2-5c46893bcefb",
   "metadata": {},
   "source": [
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Background Subtraction Parameters for `cv2.createBackgroundSubtractorMOG2()`\r\n",
    "\r\n",
    "The `cv2.createBackgroundSubtractorMOG2()` function in OpenCV is used to create a background subtractor object based on the MOG2 algorithm. This function takes three arguments:\r\n",
    "\r\n",
    "1. **detectShadows (Boolean)**:\r\n",
    "   - This parameter enables or disables shadow detection.\r\n",
    "   - When set to `True`, the algorithm detects shadows in the video.\r\n",
    "   - Shadow detection can provide smoother and more robust results, but it may slightly decrease processing speed.\r\n",
    "\r\n",
    "2. **history (Integer)**:\r\n",
    "   - The `history` parameter determines the number of previous frames used to create the background model.\r\n",
    "   - Increasing this value can be beneficial if the target object frequently stops or pauses in the video.\r\n",
    "\r\n",
    "3. **Threshold Limit (Integer)**:\r\n",
    "   - The threshold limit helps filter out noise present in the frame.\r\n",
    "   - Increasing this value can help remove noise, such as white spots in the frame.\r\n",
    "   - Additionally, morphological operations like erosion can further eliminate noise from the frame.\r\n",
    "\r\n",
    "These parameters are essential for fine-tuning the background subtraction process and improving the accuracy of object detection itebook for easy reference.**\n",
    "\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11ef3d09-db0d-4da0-9718-af9e9166ba8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< cv2.BackgroundSubtractorMOG2 0000021F576324D0>\n"
     ]
    }
   ],
   "source": [
    "# For loading the video\n",
    "cap = cv2.VideoCapture('sample_video1.mp4') \n",
    "\n",
    "# Create the background subtractor object\n",
    "flog = cv2.createBackgroundSubtractorMOG2(detectShadows = True, varThreshold = 50, history = 2800)\n",
    "print(flog)\n",
    "\n",
    "while(True): \n",
    "    ret,frame = cap.read() \n",
    "\n",
    "    if not ret: \n",
    "        break\n",
    "\n",
    "     # Apply the background object on each frame\n",
    "    fgmask = flog.apply(frame) \n",
    "\n",
    "    # Get rid of the shadows\n",
    "    ret, fgmask = cv2.threshold(fgmask, 250, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "     # Show the background subtraction frame.\n",
    "    cv2.imshow('All three',fgmask)\n",
    "    k = cv2.waitKey(10)\n",
    "    if k == 27: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf06e5c-7d02-4821-b75e-aef2788c2443",
   "metadata": {},
   "source": [
    "# adding the dilation and erosion in this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec218486-15c5-45ee-96c5-6a0f0f71556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('sample_video1.mp4') \n",
    "\n",
    "# initialising the background substractor object \n",
    "backGroundSubstractorObject = cv2.createBackgroundSubtractorMOG2() \n",
    "thresh = 1100 \n",
    "\n",
    "# noise filter thresold \n",
    "kernel= None\n",
    "\n",
    "while(1): \n",
    "    ret,frame = cap.read() \n",
    "\n",
    "    if not ret: \n",
    "        break \n",
    "\n",
    "    # applying the dilation and erosion to the images\n",
    "    fgmask = backGroundSubstractorObject.apply(frame) \n",
    "\n",
    "    # Get rid of the shadows\n",
    "    ret, fgmask = cv2.threshold(fgmask, 250, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    \n",
    "    fgmask = cv2.erode(fgmask,kernel,iterations = 1)\n",
    "    fgmask = cv2.dilate(fgmask,kernel,iterations = 4)\n",
    "\n",
    "\n",
    "    # Detection of the contours in the frame \n",
    "    contours, hierarchy = cv2.findContours(fgmask,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    #  Contours in OpenCV refer to the outlines or boundaries of objects detected in an image,  \n",
    "    # represented as a series of connected points. They are useful for tasks such as object detection, shape analysis, and image segmentation.\n",
    "\n",
    "    if contours: \n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "        # This line of code finds the contour with the maximum area among the list of contours. The cv2.contourArea() function calculates the area of each contour, \n",
    "        # and max() is used to find the contour with the maximum area based on the values returned by cv2.contourArea(). The parameter key=cv2.contourArea specifies \n",
    "        # that the maximum value should be determined based on the area of the contours.\n",
    "        # Make sure the contour area is somewhat higher than a threshold to ensure it's a person and not noise.\n",
    "        \n",
    "        if cv2.contourArea(cnt) > thresh:\n",
    "            # Draw a bounding box around the person and label it as person detected\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "            cv2.putText(frame, 'Person Detected', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "        # Stack both frames and show the image\n",
    "    fgmask_3 = cv2.cvtColor(fgmask, cv2.COLOR_GRAY2BGR)\n",
    "    stacked = np.hstack((fgmask_3, frame))\n",
    "    # Adjust window size\n",
    "    cv2.imshow('Combined', cv2.resize(stacked, (800, 600)))\n",
    "\n",
    "    k = cv2.waitKey(40) \n",
    "    if k == ord('q'):\n",
    "        break\n",
    "        \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b93595-3675-4328-be37-9c5bcb180d62",
   "metadata": {},
   "source": [
    "# Step 4: Final code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf2d6fb-1269-4279-bdb2-a3ae698dbbe2",
   "metadata": {},
   "source": [
    "## Defining user defined function for the person detection and one function for the sending the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5137da5f-1e95-4e72-bc7b-d76aa5afdf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_person_present(frame, thresh=1100):\n",
    "    kernel = None\n",
    "    global foog\n",
    "    \n",
    "    # Apply background subtraction\n",
    "    fgmask = foog.apply(frame)\n",
    "\n",
    "    # Get rid of the shadows\n",
    "    ret, fgmask = cv2.threshold(fgmask, 250, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Apply some morphological operations to make sure you have a good mask\n",
    "    fgmask = cv2.dilate(fgmask, kernel, iterations=4)\n",
    "\n",
    "    # Detect contours in the frame\n",
    "    contours, hierarchy = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "     \n",
    "    # Check if there was a contour and the area is somewhat higher than some threshold so we know it's a person and not noise\n",
    "    if contours and cv2.contourArea(max(contours, key=cv2.contourArea)) > thresh:\n",
    "            \n",
    "        # Get the max contour\n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Draw a bounding box around the person and label it as person detected\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, 'Person Detected', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "        \n",
    "        return True, frame\n",
    "        \n",
    "        \n",
    "    # Otherwise report there was no one present\n",
    "    else:\n",
    "        return False, frame\n",
    "\n",
    "\n",
    "def send_message(body, info_dict):\n",
    "\n",
    "    # Your Account SID from twilio.com/console\n",
    "    account_sid = info_dict['account_sid']\n",
    "\n",
    "    # Your Auth Token from twilio.com/console\n",
    "    auth_token  = info_dict['auth_token']\n",
    "\n",
    "\n",
    "    client = Client(account_sid, auth_token)\n",
    "\n",
    "    message = client.messages.create(to=info_dict['your_num'], from_=info_dict['trial_num'], body=body)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_percentage_change(frame1, frame2):\n",
    "    # Convert frames to grayscale\n",
    "    gray_frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate absolute difference between the two frames\n",
    "    abs_diff = cv2.absdiff(gray_frame1, gray_frame2)\n",
    "\n",
    "    # Calculate percentage change\n",
    "    percentage_change = (np.count_nonzero(abs_diff) / abs_diff.size) * 100\n",
    "\n",
    "    return percentage_change\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8949b-4fa4-465e-9a13-24db65bb17fd",
   "metadata": {},
   "source": [
    "# Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f027d454-57af-41ec-b360-ca949e3cd39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set Window normal so we can resize it\n",
    "cv2.namedWindow('frame', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# This is a test video\n",
    "cap = cv2.VideoCapture('sample_video1.mp4')\n",
    "\n",
    "# Read the video stream from the camera\n",
    "# cap = cv2.VideoCapture('http://192.168.43.1:8080/video')\n",
    "\n",
    "# Get width and height of the frame\n",
    "width = int(cap.get(3))\n",
    "height = int(cap.get(4))\n",
    "\n",
    "# Read and store the credentials information in a dict\n",
    "with open('credential.txt', 'r') as myfile:\n",
    "  data = myfile.read()\n",
    "\n",
    "info_dict = eval(data)\n",
    "\n",
    "# Initialize the background Subtractor\n",
    "foog = cv2.createBackgroundSubtractorMOG2(detectShadows=True, varThreshold=100, history=2000)\n",
    "\n",
    "# Status is True when person is present and False when the person is not present.\n",
    "status = False\n",
    "\n",
    "# After the person disappears from view, wait at least 7 seconds before making the status False\n",
    "patience = 0.099\n",
    "\n",
    "# We don't consider an initial detection unless it's detected 15 times, this gets rid of false positives\n",
    "detection_thresh = 5\n",
    "\n",
    "# Initial time for calculating if patience time is up\n",
    "initial_time = None\n",
    "\n",
    "\n",
    "\n",
    "# We are creating a deque object of length detection_thresh and will store individual detection statuses here\n",
    "de = deque([False] * detection_thresh, maxlen=detection_thresh)\n",
    "\n",
    "# Initialize an empty heap to store frames along with their negated percentage change\n",
    "frame_heap = []\n",
    "\n",
    "\n",
    "# Initialize these variables for calculating FPS\n",
    "fps = 0 \n",
    "frame_counter = 0\n",
    "start_time = time.time()\n",
    "counter = 0\n",
    "\n",
    "ret,prevframe = cap.read()\n",
    "initial_frame = prevframe\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break \n",
    "    \n",
    "            \n",
    "    # This function will return a boolean variable telling if someone was present or not, it will also draw boxes if it \n",
    "    # finds someone\n",
    "    detected, annotated_image = is_person_present(frame)  \n",
    "    \n",
    "    # Register the current detection status on our deque object\n",
    "    de.appendleft(detected)\n",
    "    \n",
    "    # If we have consecutively detected a person 15 times then we are sure that someone is present    \n",
    "    # We also make this is the first time that this person has been detected so we only initialize the videowriter once\n",
    "    if sum(de) == detection_thresh and not status:                       \n",
    "            status = True\n",
    "            entry_time = datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")\n",
    "            # out = cv2.VideoWriter('outputs/{}.mp4'.format(entry_time), cv2.VideoWriter_fourcc(*'XVID'), 15.0, (width, height))\n",
    "\n",
    "    # If status is True but the person is not in the current frame\n",
    "    if status and not detected:\n",
    "        \n",
    "        # Restart the patience timer only if the person has not been detected for a few frames so we are sure it wasn't a \n",
    "        # False positive\n",
    "        print('i reached deepest of the darkest point')\n",
    "        if sum(de) > (detection_thresh/2): \n",
    "            if initial_time is None:\n",
    "                print('initial time as been setted to',initial_time)\n",
    "                initial_time = time.time()\n",
    "                \n",
    "            \n",
    "            elif initial_time is not None:        \n",
    "                print('now the initial time is not null beacuse current time is ',initial_time)\n",
    "                print(time.time() - initial_time)\n",
    "                # If the patience has run out and the person is still not detected then set the status to False\n",
    "                # Also save the video by releasing the video writer and send a text message.\n",
    "                if  time.time() - initial_time >= patience:\n",
    "                    status = False\n",
    "                    exit_time = datetime.datetime.now().strftime(\"%A, %I:%M:%S %p %d %B %Y\")\n",
    "                    # out.release()\n",
    "                    # print(out)\n",
    "                    initial_time = None\n",
    "                    de = deque([False] * len(de))\n",
    "                    body = \"Alert: \\nA Person Entered the Room at {} \\nLeft the room at {}\".format(entry_time, exit_time)\n",
    "                    print('message has been sent')\n",
    "                    send_message(body, info_dict)\n",
    "                    \n",
    "                \n",
    "    \n",
    "    # If a significant amount of detections (more than half of detection_thresh) has occurred then we reset the Initial Time.\n",
    "    elif status and sum(de) > (detection_thresh/2):\n",
    "        # print('person has detected beyond half thresold limit')\n",
    "        # print(sum(de))\n",
    "        change_percentage = calculate_percentage_change(prevframe, frame)\n",
    "        change_initial_change = calculate_percentage_change(initial_frame,frame)\n",
    "        if change_initial_change > 60: \n",
    "            heapq.heappush(frame_heap, (-change_percentage, frame,datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")))\n",
    "        prevframe = frame\n",
    "        initial_time = None\n",
    "    \n",
    "    # Get the current time in the required format\n",
    "    current_time = datetime.datetime.now().strftime(\"%A, %I:%M:%S %p %d %B %Y\")\n",
    "\n",
    "    # Display the FPS\n",
    "    cv2.putText(annotated_image, 'FPS: {:.2f}'.format(fps), (510, 450), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 40, 155), 2)\n",
    "    \n",
    "    # Display Time\n",
    "    cv2.putText(annotated_image, current_time, (310, 20), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1)    \n",
    "    \n",
    "    # Display the Room Status\n",
    "    cv2.putText(annotated_image, 'Room Occupied: {}'.format(str(status)), (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, \n",
    "                (200, 10, 150), 2)\n",
    "\n",
    "    # Show the patience Value\n",
    "    if initial_time is None:\n",
    "        text = 'Patience: {}'.format(patience)\n",
    "    else: \n",
    "        text = 'Patience: {:.2f}'.format(max(0, patience - (time.time() - initial_time)))\n",
    "        \n",
    "    cv2.putText(annotated_image, text, (10, 450), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 40, 155), 2)   \n",
    "\n",
    "    # If status is true save the frame\n",
    "    # if status:\n",
    "    #     out.write(annotated_image)\n",
    " \n",
    "    # Show the Frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    \n",
    "    # Calculate the Average FPS\n",
    "    frame_counter += 1\n",
    "    fps = (frame_counter / (time.time() - start_time))\n",
    "    \n",
    "    \n",
    "    # Exit if q is pressed.\n",
    "    if cv2.waitKey(30) == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "# Release Capture and destroy windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "frame_index = 0\n",
    "while frame_heap:\n",
    "    # Pop the frame with the maximum percentage change (negated)\n",
    "    neg_change_percentage, frame , date = heapq.heappop(frame_heap)\n",
    "    change_percentage = -neg_change_percentage\n",
    "    file_path = 'DetectedPerson'\n",
    "    # Save the frame to disk\n",
    "    cv2.imwrite(f'{file_path}/{date}.jpg', frame)\n",
    "    frame_index += 1\n",
    "# out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f56371-02f6-45de-b5d5-ff64623f8bd8",
   "metadata": {},
   "source": [
    "# Face Recognisation Code: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c56b1b-ad5a-4116-b362-412697630090",
   "metadata": {},
   "source": [
    "Post Link: https://data-flair.training/blogs/python-face-recognition/ \n",
    "\n",
    "Dlib installation Link:  https://www.youtube.com/watch?v=pHrgi8QLcKk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42400fe-ccbb-4354-803f-9d2ae567edaf",
   "metadata": {},
   "source": [
    "# Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bce0df61-0607-4c24-b4b8-a16767bd73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a1c8d3-5878-4a9d-8f8b-e39a62e8ba6e",
   "metadata": {},
   "source": [
    "# Code for Getting the person Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df72e924-4298-4f51-bbe6-fe24530a3482",
   "metadata": {},
   "outputs": [],
   "source": [
    "haar_file=cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "\n",
    "path=r'E:/MachineLearning/SmartIntruderDetectionSystem/I Am Loki/.ipynb_checkpoints/Face_Data/Wanda'\n",
    "width=130\n",
    "height=100  #saved image size = 130 x 100\n",
    "face_cascade = cv2.CascadeClassifier(haar_file)\n",
    "webcam=cv2.VideoCapture(0)\n",
    "\n",
    "count=0\n",
    "while count < 50:\n",
    "    i,im=webcam.read()\n",
    "    gray=cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_cascade.detectMultiScale(gray,1.3,4)\n",
    "    for (p,q,r,s) in faces:\n",
    "        cv2.rectangle(im,(p,q),(p+r,q+s),(255,0,0),2)\n",
    "        face=gray[q:q+s,p:p+r]\n",
    "        face_resize=cv2.resize(face,(width,height))\n",
    "        cv2.imwrite('%s/%s.png' % (path,count),face_resize)\n",
    "    count+=1\n",
    "    cv2.imshow('Saving DATA',im)\n",
    "    # cv2.imshow('Crop view',face)\n",
    "    cv2.waitKey(10)\n",
    "\n",
    "webcam.release()\n",
    "# Destroy the windows you created\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b203fe35-07e8-4ff2-a266-6008c6f8466d",
   "metadata": {},
   "source": [
    "# Code for recognising the person from our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f25d2629-74e4-4265-839a-a2ac62c23d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 9\n",
      "Line 25\n",
      "Line 29\n",
      "line 32\n",
      "I am Home lander\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n",
      "deteccted\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     40\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m---> 41\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mface_cascade\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectMultiScale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x, y, w, h) \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[0;32m     43\u001b[0m     face \u001b[38;5;241m=\u001b[39m gray[y:y \u001b[38;5;241m+\u001b[39m h, x:x \u001b[38;5;241m+\u001b[39m w]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "os.chdir(r'E:/MachineLearning/SmartIntruderDetectionSystem/I Am Loki/.ipynb_checkpoints')\n",
    "haar_file = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "datasets = 'Face_Data'\n",
    "\n",
    "print('Line 9')\n",
    "# Create a list of images and a list of corresponding names\n",
    "(images, labels, names, id) = ([], [], {}, 0)\n",
    "for (subdirs, dirs, files) in os.walk(datasets):\n",
    "    for subdir in dirs:\n",
    "        names[id] = subdir\n",
    "        subjectpath = os.path.join(datasets, subdir)\n",
    "        for filename in os.listdir(subjectpath):\n",
    "            path = os.path.join(subjectpath, filename)\n",
    "            label = id\n",
    "            images.append(cv2.imread(path, 0))\n",
    "            labels.append(int(label))\n",
    "        id += 1\n",
    "\n",
    "width, height = 130, 100\n",
    "\n",
    "print('Line 25')\n",
    "# Create a Numpy array \n",
    "(images, labels) = [numpy.array(lst) for lst in [images, labels]]\n",
    "\n",
    "print('Line 29')\n",
    "# Train Model\n",
    "model = cv2.face.LBPHFaceRecognizer_create()\n",
    "print('line 32')\n",
    "model.train(images, labels)\n",
    "\n",
    "print('I am Home lander')\n",
    "# Face Recognition \n",
    "face_cascade = cv2.CascadeClassifier(haar_file)\n",
    "webcam = cv2.VideoCapture(0)\n",
    "c = 1\n",
    "while True:\n",
    "    ret,frame = webcam.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 4)\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = gray[y:y + h, x:x + w]\n",
    "        face_resize = cv2.resize(face, (width, height))\n",
    "        # Try to recognize the face\n",
    "        prediction = model.predict(face_resize)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        if prediction[1] < 500:\n",
    "            print('deteccted')\n",
    "            cv2.putText(frame, '%s' % (names[prediction[0]]), (x-10, y-10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2, (0, 255, 0))\n",
    "        else:\n",
    "            cv2.putText(frame, 'not recognized', (x-10, y-10), cv2.FONT_HERSHEY_PLAIN, 1, (0, 255, 0))\n",
    "        \n",
    "    # Display image using matplotlib\n",
    "    # print(im)\n",
    "    cv2.imshow('recognised',frame)\n",
    "    key = cv2.waitKey(10)\n",
    "    if key == 'a':\n",
    "        break\n",
    "\n",
    "webcam.release()\n",
    "# Destroy the windows you created\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e91705-261a-4bfe-b910-a6180444aebd",
   "metadata": {},
   "source": [
    "# Integrating our code with facial recognition code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca148dd5-221a-4838-8eca-6e0a06621660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_person_present(frame, thresh=1100):\n",
    "    kernel = None\n",
    "    global foog\n",
    "    \n",
    "    # Apply background subtraction\n",
    "    fgmask = foog.apply(frame)\n",
    "\n",
    "    # Get rid of the shadows\n",
    "    ret, fgmask = cv2.threshold(fgmask, 250, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Apply some morphological operations to make sure you have a good mask\n",
    "    fgmask = cv2.dilate(fgmask, kernel, iterations=4)\n",
    "\n",
    "    # Detect contours in the frame\n",
    "    contours, hierarchy = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "     \n",
    "    # Check if there was a contour and the area is somewhat higher than some threshold so we know it's a person and not noise\n",
    "    if contours and cv2.contourArea(max(contours, key=cv2.contourArea)) > thresh:\n",
    "            \n",
    "        # Get the max contour\n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Draw a bounding box around the person and label it as person detected\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, 'Person Detected', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "        \n",
    "        return True, frame\n",
    "        \n",
    "        \n",
    "    # Otherwise report there was no one present\n",
    "    else:\n",
    "        return False, frame\n",
    "\n",
    "\n",
    "def send_message(body, info_dict):\n",
    "\n",
    "    # Your Account SID from twilio.com/console\n",
    "    account_sid = info_dict['account_sid']\n",
    "\n",
    "    # Your Auth Token from twilio.com/console\n",
    "    auth_token  = info_dict['auth_token']\n",
    "\n",
    "\n",
    "    client = Client(account_sid, auth_token)\n",
    "\n",
    "    message = client.messages.create(to=info_dict['your_num'], from_=info_dict['trial_num'], body=body)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_percentage_change(frame1, frame2):\n",
    "    # Convert frames to grayscale\n",
    "    gray_frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate absolute difference between the two frames\n",
    "    abs_diff = cv2.absdiff(gray_frame1, gray_frame2)\n",
    "\n",
    "    # Calculate percentage change\n",
    "    percentage_change = (np.count_nonzero(abs_diff) / abs_diff.size) * 100\n",
    "\n",
    "    return percentage_change\n",
    "\n",
    "\n",
    "def recogniseFaceFromFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd738b1c-c42c-45b6-af2c-f3e7c9a0a2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf6a15-ddd5-4c32-9042-71b57c3460f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f321573-f2da-40e6-8cbc-f123b429efeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ba24e8-1147-4a68-a13c-8a5479d7bd55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
